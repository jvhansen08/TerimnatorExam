# Question 5

## Instructions to Run

Be sure to use the installation [instructions](../README.md) found in the root of this repository to install the necessary dependencies. Once they are installed in a virtual env, you can run the simulation.

To run the catpole simulation, execute this command from the within the `Q5` directory:

```bash
python3 main.py
```

To run the frozen lake simulation, execute this command from the within the `Q5` directory:

```bash
python3 frozenlake.py
```

The questions are answered below.

## Part A

Explain three merits and three demerits of using reinforcement learning for mechatronic systems.

### Merits

1. Adaptive Control: RL enables mechatronic systems to learn and adapt to changing environments and requirements. This adaptability is valuable in scenarios where traditional control methods may struggle to cope with dynamic or uncertain conditions.

2. Autonomous Decision-Making: Mechatronic systems equipped with RL can make autonomous decisions to achieve specific goals. This is particularly useful in applications such as robotics, where the system must navigate and interact with its environment without constant human intervention.

3. Complex System Optimization: RL can be used to optimize complex, non-linear systems with high-dimensional state and action spaces. It can learn optimal control policies that maximize rewards, making it suitable for tasks like optimizing the energy efficiency of a mechatronic system or fine-tuning controller parameters.

### Demerits

1. High Computational Requirements: Training RL models can be computationally intensive, requiring significant computational resources, time, and data. This can be a practical limitation, especially for real-time or resource-constrained mechatronic systems.
2. Limited Sample Efficiency: RL algorithms often require a substantial amount of trial-and-error exploration to learn effective policies. In some cases, this extensive exploration can be impractical or even dangerous, particularly in applications where mistakes can lead to damage or safety hazards.
3. Lack of Guaranteed Stability: Traditional control methods often come with theoretical guarantees of stability and performance. RL, on the other hand, typically lacks such guarantees. Ensuring the stability of a mechatronic system using RL can be challenging, and additional safety mechanisms may be necessary.

## Part B

Draw a diagram that contrasts reinforcement learning and controls.

![RL vs Controls](./images/Reinforcement_control_comparison.png)

## Part C

`rightForcefulCartpole.py` contains the cartpole simulation with the right aggressive force as 50% more than the left.

See the code for both the frozen lake and the cartpole with an aggressive right in this directory.

### Comparing the Normal Frozen Lake with the Random Death Frozen Lake

Here are the results for the normal frozen lake:

![Normal Lake Results](./images/frozenlake_states_actions_distrib_8x8.png)

![Normal Lake Map](./images/frozenlake_q_values_8x8.png)

And the results for the random death frozen lake:

![Random Death Lake Results](./images/random_death_frozenlake_states_actions_distrib_8x8.png)
![Random Death Lake Map](./images/random_death_map.png)

As you can see the number of counts that ended inf the first 10 steps is 100,000 higher than on the normal lake. This is because the agent is randomly dying and being reset. The Q values are also generally lower because the agent is dying faster and can't learn as much.

They do both have a similar number that reach 60 states, but the random death lake is lower.

So, the outcomes can be similar in that both models find the goal, but the random death lake requires many more agents to arrive there because of the many deaths that occur.
