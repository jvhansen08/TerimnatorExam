## Part A:
#### Merits of the Algorithm
* It's distinct from the other algorithms used in this midterm. While the other algorithms deal with either forming a map or recognizing objects, this algorithm focuses on judging the velocity of oncoming objects
* Of course, it uses elements present in the other algorithms - i.e. image recognition - but utilizes them in a distinct application
* I will note, one obvious downside of the algorithm is the difficulty of implementation. If they had provided a docker image or some similar aid, this algorithm would be much more approachable.

#### How it works
* Essentially, this algorithm goes frame-by-frame in the provided input and searches for certain, key features.
* When these features are found, they are identified, and a bounding box is provided.
* This algorithm is then able to look at a sequence of frames, see how much a given object has shifted from one frame to the next, and calculate the distance traversed
* Importantly, these calculations are made based on estimates of how far away an object is (for example, something shifting left by 3 pixels indicates a much higher velocity for an object that is 300 feet away than for an object that is 3 feet away).
* Once the distance from the camera to the object - and the distance between where the object was and where it is now - are known, it is relatively simple calculation to judge the velocity based on the time elapsed.

Due to time and configuration constraints, we couldn't get this program to run to completion. Below is a list of what we tried:

* Ran 'python train_crop_velocity.py' - resulted in ModuleNotFoundError: No module named 'correlation_cuda'
* Downloaded cuda for windows
* Navigated to networks/correlation and ran 'python setup.py build' - resulted in   File "C:\Python311\Lib\site-packages\pkg_resources\_vendor\packaging\version.py", line 264, in __init__
    match = self._regex.search(version)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: expected string or bytes-like object, got 'NoneType'
* Edited C:\Python311\Lib\site-packages\pkg_resources\_vendor\packaging\version.py to be self._regex.search(str(version))
* Re-ran 'python setup.py build' - resulted in RuntimeError:
The detected CUDA version (12.3) mismatches the version that was used to compile
PyTorch (None). Please make sure to use the same CUDA versions.
* Uninstalled torch and reinstalled it, got: AttributeError: 'LegacyVersion' object has no attribute 'major'
* Decided to try WSL: installed cuda and torch (torch broke wsl, had to restart computer, was successfully installed on 2nd try).
* Ran 'python setup.py build' - result: 'raise RuntimeError(message) from e
RuntimeError: Error compiling objects for extension
* Pulled out old laptop with Linux Mint, downloaded repo, downloaded dependencies, ran 'python setup.py build'. Result: 'raise RuntimeError(message) from e
RuntimeError: Error compiling objects for extension


## Part B:
It took a while to get this to work properly - ultimately, we found success when using opencv-python version 3.4.17.61. Additionally, contrary to advice we received, it was not sufficient to provide the first line of pose data - the program expected pose data per frame. Given that none of us has a camera that generates pose data, and any attempt to recreate it would be marginally successful at best, we decided to remove that component of the program altogether. The path taken was a rectangle.

The video used for this question is too large to upload to github, so I have published it on my google drive account, and it is accessible [here](https://drive.google.com/file/d/1UhxoTC_4qIo-FAB6tTEa8wwIuOE9bcX_/view?usp=sharing)

Another obstacle we encountered was that, for some reason, some of the frames caused the program to crash. We automated the process of removing frames, and in all, about 100 of the ~12,000 frames were deleted. This may have marginally contributed to the inacuraccy of the map shown below.

More than that, however, I think the inacuraccy of this map is due to the limitation of monocular odometry in general. I used a relatively high-resolution camera (1920x1080) with a relatively high frame rate(30 fps). I walked in a regular path, and held the camera fairly steady. I would suspect that, for the most part, the map missed the mark because monocular odometry is, without any other points of reference, inaccurate to begin with.

It is possible that the venue of the video futher contributed to the map's inaccuracy. I recorded the video in the cemetary, where the majority of the landmarks are trees of a similar size and shape. I would have expected this algorithm to be able to compensate for that, but it is possible the environment was a contributing factor.

Below, you can find both the original trajectory image, and below that you can see the original trajectory image overlayed with an approximation of the actual path taken.

 ![original trajectory](./trajectory.png)

 ![original trajectory with actual path overlaid](./trajectory_actual_path.png)


Note: Given that I implemented both parts on a separate Linux machine, I have not included the implementations (as they are not specifically requested by the question) in this repo. I would be happy to show the program running on that machine, if so desired.
